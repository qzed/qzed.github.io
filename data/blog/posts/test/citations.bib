
@inproceedings{abbeelApprenticeshipLearningInverse2004,
  title = {Apprenticeship Learning via Inverse Reinforcement Learning},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Abbeel, Pieter and Ng, Andrew Y.},
  date = {2004},
  publisher = {{ACM Press}},
  location = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015430},
  url = {http://portal.acm.org/citation.cfm?doid=1015330.1015430},
  urldate = {2021-06-22},
  abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using “inverse reinforcement learning” to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert’s reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert’s unknown reward function.},
  eventtitle = {Twenty-First International Conference},
  langid = {english},
  file = {/home/luzmn/Documents/Papers/Abbeel and Ng - 2004 - Apprenticeship learning via inverse reinforcement .pdf}
}

@article{aliVisualObjectTracking2016,
  title = {Visual Object Tracking — Classical and Contemporary Approaches},
  author = {Ali, Ahmad and Jalil, Abdul and Niu, Jianwei and Zhao, Xiaoke and Rathore, Saima and Ahmed, Javed and Aksam Iftikhar, Muhammad},
  date = {2016-02-01},
  journaltitle = {Frontiers of Computer Science},
  shortjournal = {Front. Comput. Sci.},
  volume = {10},
  number = {1},
  pages = {167--188},
  issn = {2095-2236},
  doi = {10.1007/s11704-015-4246-3},
  url = {https://doi.org/10.1007/s11704-015-4246-3},
  urldate = {2022-01-18},
  abstract = {Visual object tracking (VOT) is an important subfield of computer vision. It has widespread application domains, and has been considered as an important part of surveillance and security system. VOA facilitates finding the position of target in image coordinates of video frames.While doing this, VOA also faces many challenges such as noise, clutter, occlusion, rapid change in object appearances, highly maneuvered (complex) object motion, illumination changes. In recent years, VOT has made significant progress due to availability of low-cost high-quality video cameras as well as fast computational resources, and many modern techniques have been proposed to handle the challenges faced by VOT. This article introduces the readers to 1) VOT and its applications in other domains, 2) different issues which arise in it, 3) various classical as well as contemporary approaches for object tracking, 4) evaluation methodologies for VOT, and 5) online resources, i.e., annotated datasets and source code available for various tracking techniques.},
  langid = {english},
  file = {/home/luzmn/Documents/Papers/Ali et al. - 2016 - Visual object tracking—classical and contemporary .pdf}
}

@inproceedings{ziebartMaximumEntropyInverse2008,
  title = {Maximum {{Entropy Inverse Reinforcement Learning}}},
  booktitle = {Proceedings of the 23rd {{National Conference}} on {{Artificial Intelligence}} - {{Volume}} 3},
  author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
  date = {2008-07-13},
  series = {{{AAAI}}'08},
  pages = {1433--1438},
  publisher = {{AAAI Press}},
  location = {{Chicago, Illinois}},
  abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
  isbn = {978-1-57735-368-3},
  langid = {english},
  file = {/home/luzmn/Documents/Papers/Ziebart et al. - 2008 - Maximum Entropy Inverse Reinforcement Learning.pdf}
}

@inproceedings{ziebartPrincipleMaximumCausal2013,
  title = {The {{Principle}} of {{Maximum Causal Entropy}} for {{Estimating Interacting Processes}}},
  booktitle = {{{IEEE Transactions}} on {{Information Theory}}},
  author = {Ziebart, Brian D. and Bagnell, J. Andrew and Dey, Anind K.},
  date = {2013-04},
  volume = {59},
  pages = {1966--1980},
  publisher = {{IEEE}},
  doi = {10.1109/TIT.2012.2234824},
  url = {http://ieeexplore.ieee.org/document/6479340/},
  urldate = {2021-06-22},
  abstract = {The principle of maximum entropy provides a powerful framework for estimating joint, conditional, and marginal probability distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on directed information theory for estimating an unknown process based on its interactions with a known process. We demonstrate the breadth of the approach using two applications: a predictive solution for inverse optimal control in decision processes and computing equilibrium strategies in sequential games.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  langid = {english},
  keywords = {Causal entropy,correlated equilibrium (CE),directed information,Entropy,Estimation,inverse optimal control,inverse reinforcement learning,Joints,maximum entropy,Optimization,Probability distribution,Process control,Random variables,statistical estimation},
  annotation = {Print ISSN: 0018-9448 Electronic ISSN: 1557-9654},
  file = {/home/luzmn/Documents/Papers/Ziebart et al. - 2013 - The Principle of Maximum Causal Entropy for Estima.pdf}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

