1:"$Sreact.fragment"
2:I[22016,["/_next/static/chunks/dc82041b01df271f.js","/_next/static/chunks/3b33f78cf07ae5fc.js"],""]
3:I[48234,["/_next/static/chunks/dc82041b01df271f.js","/_next/static/chunks/3b33f78cf07ae5fc.js"],"Mdx"]
4:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"OutletBoundary"]
5:"$Sreact.suspense"
:HL["/_next/static/chunks/237a07649595a9a2.css","style"]
0:{"buildId":"8aLy86eBE0NwXpn_GtgjZ","rsc":["$","$1","c",{"children":[["$","main",null,{"children":["$","div",null,{"className":"py-8 sm:py-12 lg:py-16 px-8","children":["$","div",null,{"className":"mx-auto max-w-prose flex flex-col gap-10","children":[["$","$1","2022/08/irl-maxent",{"children":[["$","article",null,{"className":"relative group","children":[["$","$L2",null,{"href":"/blog/posts/2022/08/irl-maxent","className":"absolute inset-0 z-10","aria-label":"Maximum Entropy Inverse Reinforcement Learning","children":["$","span",null,{"className":"sr-only","children":"Maximum Entropy Inverse Reinforcement Learning"}]}],["$","div",null,{"className":"group-hover:text-sky-800 dark:group-hover:text-slate-100","children":[["$","div",null,{"className":"mb-4","children":[["$","p",null,{"className":"mb-1 text-xs sm:text-sm text-gray-500 dark:text-gray-500","children":"August 25, 2022"}],["$","div",null,{"className":"","children":[["$","h1",null,{"className":"text-xl sm:text-2xl font-bold","children":"Maximum Entropy Inverse Reinforcement Learning"}],"$undefined"]}]]}],["$","div",null,{"className":" mdprose max-w-none prose-slate dark:prose-invert prose-sm sm:prose-base  text-pretty text-optimize-legibility break-words hyphens-auto subpixel-antialiased  prose-img:rounded prose-img:border prose-img:border-slate-200 dark:prose-img:border-slate-800  prose-table:rounded prose-table:bg-slate-200 dark:prose-table:bg-slate-800  prose-th:px-3 prose-td:px-3 prose-th:py-2 prose-td:py-2  prose-thead:border-b prose-thead:border-slate-400 dark:prose-thead:border-slate-500 prose-td:border-b prose-td:border-slate-300 dark:prose-td:border-slate-700 [&>*]:last:prose-tr:border-none  first:prose-th:pl-5 first:prose-td:pl-5 last:prose-th:pr-5 last:prose-td:pr-5  prose-code:font-normal ","children":["$","$L3",null,{"code":"const{jsx:e}=arguments[0];function _createMdxContent(n){const r={p:\"p\",...n.components};return e(r.p,{children:\"Reinforcement learning (RL) aims to provide a framework for finding the\\noptimal behavior of an intelligent agent acting in some environment.  With\\nthe help of inverse reinforcement learning (IRL) we can try to improve our\\nagents by recovering the behavior of an expert as a reward function, in\\nessence using its domain knowledge for our needs. Maximum entropy IRL is a\\ncomparatively simple but clever method of solving the general IRL problem\\nfor discrete Markov decision processes.\"})}return{default:function(n={}){const{wrapper:r}=n.components||{};return r?e(r,{...n,children:e(_createMdxContent,{...n})}):_createMdxContent(n)}};"}]}]]}]]}],["$","hr",null,{"className":"last:hidden h-px bg-slate-200 dark:bg-slate-800 border-0"}]]}]]}]}]}],[["$","link","0",{"rel":"stylesheet","href":"/_next/static/chunks/237a07649595a9a2.css","precedence":"next"}],["$","script","script-0",{"src":"/_next/static/chunks/3b33f78cf07ae5fc.js","async":true}]],["$","$L4",null,{"children":["$","$5",null,{"name":"Next.MetadataOutlet","children":"$@6"}]}]]}],"loading":null,"isPartial":false}
6:null
