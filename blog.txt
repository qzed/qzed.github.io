1:HL["/_next/static/css/5c88f543143846fd.css","style",{"crossOrigin":""}]
0:["UxthWcg1kA9Wix-OZvGff",[[["",{"children":["blog",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],"$L2",[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/5c88f543143846fd.css","precedence":"next","crossOrigin":""}]],"$L3"]]]]
4:HL["/_next/static/css/1689d629e66146b2.css","style",{"crossOrigin":""}]
5:HL["/_next/static/css/e05f8db85aa918f4.css","style",{"crossOrigin":""}]
6:HL["/_next/static/css/81db15d06f37591a.css","style",{"crossOrigin":""}]
7:I[6954,[],""]
8:I[7264,[],""]
9:I[1444,["326","static/chunks/326-976d427cb6019efb.js","768","static/chunks/app/blog/layout-f542a503fde4d87a.js"],""]
3:[["$","meta","0",{"charSet":"utf-8"}],["$","title","1",{"children":"Blog | Maximilian Luz"}],["$","meta","2",{"name":"description","content":"Blog of Maximilian Luz"}],["$","meta","3",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
2:[null,["$","html",null,{"lang":"en","children":["$","body",null,{"className":"home_base__hHa1D","children":[["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":["$","main",null,{"children":[["$","$L9",null,{}],["$","div",null,{"className":"not-found_section__DOGpD not-found_error__OPrrT","children":["$","div",null,{"className":"not-found_column__UyRkF","children":["$","div",null,{"className":"not-found_text__JVLHJ","children":[["$","p",null,{"className":"not-found_errorcode__eS_iR","children":"404"}],["$","p",null,{"className":"not-found_errortext__rQxgq","children":"Page not found!"}]]}]}]}]]}],"notFoundStyles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/0fa794d8593e1ce3.css","precedence":"next","crossOrigin":""}]],"childProp":{"current":[null,["$","html",null,{"lang":"en","children":["$","body",null,{"className":"home_base__hHa1D","children":[["$","$L9",null,{}],["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"loading":"$undefined","loadingStyles":"$undefined","hasLoading":false,"error":"$undefined","errorStyles":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","childProp":{"current":["$La","$Lb",null],"segment":"__PAGE__"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/e05f8db85aa918f4.css","precedence":"next","crossOrigin":""}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/81db15d06f37591a.css","precedence":"next","crossOrigin":""}]]}],["$","footer",null,{"className":"footer_footer__YtHeK","children":"Copyright © 2023 Maximilian Luz"}]]}]}],null],"segment":"blog"},"styles":[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/1689d629e66146b2.css","precedence":"next","crossOrigin":""}]]}],["$","footer",null,{"className":"footer_footer__YtHeK","children":"Copyright © 2023 Maximilian Luz"}]]}]}],null]
a:null
c:I[8326,["326","static/chunks/326-976d427cb6019efb.js","404","static/chunks/app/blog/page-a6b3ec62a07ccebd.js"],""]
b:["$","main",null,{"children":[["$","div","20220825-irl-maxent",{"className":"styles_post__X_vcU","children":["$","div",null,{"className":"styles_column__Rpho_","children":["$","div",null,{"className":"styles_text__9QAN_","children":["$","div",null,{"children":["$","div",null,{"children":[["$","div",null,{"className":"styles_title__tHXpB","children":["$","h1",null,{"children":["$","$Lc",null,{"href":"/blog/posts/20220825-irl-maxent","children":"Maximum Entropy Inverse Reinforcement Learning"}]}]}],["$","div",null,{"className":"styles_meta__o8CyI","children":[["$","p",null,{"className":"styles_date__FN_ra","children":"August 25, 2022"}],["$","div",null,{"className":"styles_tags__HYypl","children":[["$","p","machine-learning",{"className":"styles_tag__1iSz6","children":["#","machine-learning"]}],["$","p","reinforcement-learning",{"className":"styles_tag__1iSz6","children":["#","reinforcement-learning"]}],["$","p","inverse-reinforcement-learning",{"className":"styles_tag__1iSz6","children":["#","inverse-reinforcement-learning"]}]]}]]}],["$","div",null,{"className":"markdown_markdown__o1E4A","children":[["$","p",null,{"children":[["$","em",null,{"children":"Reinforcement learning (RL)"}]," aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.\nWith the help of ",["$","em",null,{"children":"inverse reinforcement learning (IRL)"}]," we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs."]}],"\n",["$","p",null,{"children":[["$","em",null,{"children":"Maximum entropy IRL"}]," is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.\nIn this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm."]}]]}],["$","div",null,{"children":["$","$Lc",null,{"href":"/blog/posts/20220825-irl-maxent","children":"Read more..."}]}]]}]}]}]}]}]]}]
