{"pageProps":{"posts":[{"slug":"20220825-irl-maxent","title":"Maximum Entropy Inverse Reinforcement Learning","author":"Maximilian Luz","date":1661385600000,"abstract":"var Component=(()=>{var g=Object.create;var s=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var v=Object.getPrototypeOf,x=Object.prototype.hasOwnProperty;var c=(n,e)=>()=>(n&&(e=n(n=0)),e);var y=(n,e)=>()=>(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=>{for(var i in e)s(n,i,{get:e[i],enumerable:!0})},l=(n,e,i,m)=>{if(e&&typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!x.call(n,r)&&r!==i&&s(n,r,{get:()=>e[r],enumerable:!(m=f(e,r))||m.enumerable});return n};var w=(n,e,i)=>(i=n!=null?g(v(n)):{},l(e||!n||!n.__esModule?s(i,\"default\",{value:n,enumerable:!0}):i,n)),j=n=>l(s({},\"__esModule\",{value:!0}),n);var o=c(()=>{});var a=c(()=>{});var p=y((X,h)=>{o();a();h.exports=_jsx_runtime});var M={};b(M,{default:()=>R});o();a();var t=w(p());function d(n){let e=Object.assign({p:\"p\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Reinforcement learning (RL)\"}),` aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.\nWith the help of `,(0,t.jsx)(e.em,{children:\"inverse reinforcement learning (IRL)\"}),\" we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Maximum entropy IRL\"}),` is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.\nIn this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.`]})]})}function L(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var R=L;return j(M);})();\n;return Component;","tags":["machine-learning","reinforcement-learning","inverse-reinforcement-learning"],"visibility":"default","content":null}]},"__N_SSG":true}