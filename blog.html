<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Maximilian Luz</title><meta name="description" content="Blog of Maximilian Luz"/><meta name="next-head-count" content="4"/><link data-next-font="" rel="preconnect" href="/" crossorigin="anonymous"/><link rel="preload" href="/_next/static/css/851187a8f50eae40.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/851187a8f50eae40.css" crossorigin="" data-n-g=""/><link rel="preload" href="/_next/static/css/e05f8db85aa918f4.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/e05f8db85aa918f4.css" crossorigin="" data-n-p=""/><link rel="preload" href="/_next/static/css/67a2fd035b910d9b.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/67a2fd035b910d9b.css" crossorigin="" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-66c7aded51d78601.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-0c7baedefba6b077.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-7af7d5359a6145de.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-4b15f0d9016180ae.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/814-170d6ee1b5f07d2b.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/blog-daf46f773f679023.js" defer="" crossorigin=""></script><script src="/_next/static/r_zq-zjkIF6Q6zUxDteYD/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/r_zq-zjkIF6Q6zUxDteYD/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><div class="styles_base__5YRdL"><nav class="navbar_base__ZGSyI"><ul class="navbar_menu__9VAe7"><li class="navbar_item__tSgzS undefined"><a href="/"><div>Home</div></a></li><li class="navbar_item__tSgzS navbar_active__qIXOm"><a href="/blog"><div>Blog</div></a></li></ul></nav><main><div class="styles_post__X_vcU"><div class="styles_column__Rpho_"><div class="styles_text__9QAN_"><div><div><div class="styles_title__tHXpB"><h1><a href="/blog/posts/20220825-irl-maxent">Maximum Entropy Inverse Reinforcement Learning</a></h1></div><div class="styles_meta__o8CyI"><p class="styles_date__FN_ra">August 25, 2022</p><div class="styles_tags__HYypl"><p class="styles_tag__1iSz6">#<!-- -->machine-learning</p><p class="styles_tag__1iSz6">#<!-- -->reinforcement-learning</p><p class="styles_tag__1iSz6">#<!-- -->inverse-reinforcement-learning</p></div></div><div class="markdown_markdown__o1E4A"><p><em>Reinforcement learning (RL)</em> aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.
With the help of <em>inverse reinforcement learning (IRL)</em> we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.</p>
<p><em>Maximum entropy IRL</em> is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.
In this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.</p></div><div><a href="/blog/posts/20220825-irl-maxent">Read more...</a></div></div></div></div></div></div></main><footer class="footer_footer__YtHeK">Copyright Â© 2023 Maximilian Luz</footer></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"posts":[{"slug":"20220825-irl-maxent","title":"Maximum Entropy Inverse Reinforcement Learning","author":"Maximilian Luz","date":1661385600000,"abstract":"var Component=(()=\u003e{var g=Object.create;var s=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var v=Object.getPrototypeOf,x=Object.prototype.hasOwnProperty;var c=(n,e)=\u003e()=\u003e(n\u0026\u0026(e=n(n=0)),e);var y=(n,e)=\u003e()=\u003e(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=\u003e{for(var i in e)s(n,i,{get:e[i],enumerable:!0})},l=(n,e,i,m)=\u003e{if(e\u0026\u0026typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!x.call(n,r)\u0026\u0026r!==i\u0026\u0026s(n,r,{get:()=\u003ee[r],enumerable:!(m=f(e,r))||m.enumerable});return n};var w=(n,e,i)=\u003e(i=n!=null?g(v(n)):{},l(e||!n||!n.__esModule?s(i,\"default\",{value:n,enumerable:!0}):i,n)),j=n=\u003el(s({},\"__esModule\",{value:!0}),n);var o=c(()=\u003e{});var a=c(()=\u003e{});var p=y((X,h)=\u003e{o();a();h.exports=_jsx_runtime});var M={};b(M,{default:()=\u003eR});o();a();var t=w(p());function d(n){let e=Object.assign({p:\"p\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Reinforcement learning (RL)\"}),` aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.\nWith the help of `,(0,t.jsx)(e.em,{children:\"inverse reinforcement learning (IRL)\"}),\" we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Maximum entropy IRL\"}),` is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.\nIn this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.`]})]})}function L(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var R=L;return j(M);})();\n;return Component;","tags":["machine-learning","reinforcement-learning","inverse-reinforcement-learning"],"visibility":"default","content":null}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"r_zq-zjkIF6Q6zUxDteYD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>