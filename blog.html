<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Maximilian Luz</title><meta name="description" content="Blog of Maximilian Luz"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/851187a8f50eae40.css" as="style"/><link rel="stylesheet" href="/_next/static/css/851187a8f50eae40.css" data-n-g=""/><link rel="preload" href="/_next/static/css/e8ae7f285bd59b72.css" as="style"/><link rel="stylesheet" href="/_next/static/css/e8ae7f285bd59b72.css" data-n-p=""/><link rel="preload" href="/_next/static/css/1ad0f733a1f452ce.css" as="style"/><link rel="stylesheet" href="/_next/static/css/1ad0f733a1f452ce.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-73b63104b83bee32.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-25e5079ab4bd6ecd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-56e54c109101a285.js" defer=""></script><script src="/_next/static/chunks/814-38d62155dfe3a73a.js" defer=""></script><script src="/_next/static/chunks/pages/blog-6f85c4977895c4e4.js" defer=""></script><script src="/_next/static/-WAzr888L1_oNiBCTp_Kl/_buildManifest.js" defer=""></script><script src="/_next/static/-WAzr888L1_oNiBCTp_Kl/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="styles_base__vlVUC"><nav class="navbar_base__qZ3rK"><ul class="navbar_menu__lmA2R"><li class="navbar_item__9W3fH undefined"><a href="/"><div>Home</div></a></li><li class="navbar_item__9W3fH navbar_active__vSOgC"><a href="/blog"><div>Blog</div></a></li></ul></nav><main><div class="styles_post__fDRxW"><div class="styles_column__I7Wru"><div class="styles_text__Fz11m"><div><div><div class="styles_title__6dIZc"><h1><a href="/blog/posts/20220825-irl-maxent">Maximum Entropy Inverse Reinforcement Learning</a></h1></div><div class="styles_meta__v7sr1"><p class="styles_date__lXmvo">August 25, 2022</p><div class="styles_tags__jXUSu"><p class="styles_tag__yYAC7">#<!-- -->machine-learning</p><p class="styles_tag__yYAC7">#<!-- -->reinforcement-learning</p><p class="styles_tag__yYAC7">#<!-- -->inverse-reinforcement-learning</p></div></div><div class="markdown_markdown__28kdH"><p><em>Reinforcement learning (RL)</em> aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.
With the help of <em>inverse reinforcement learning (IRL)</em> we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.</p>
<p><em>Maximum entropy IRL</em> is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.
In this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.</p></div><div><a href="/blog/posts/20220825-irl-maxent">Read more...</a></div></div></div></div></div></div></main><footer class="footer_footer__CKQ67">Copyright Â© 2023 Maximilian Luz</footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"20220825-irl-maxent","title":"Maximum Entropy Inverse Reinforcement Learning","author":"Maximilian Luz","date":1661385600000,"abstract":"var Component=(()=\u003e{var g=Object.create;var s=Object.defineProperty;var f=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var v=Object.getPrototypeOf,x=Object.prototype.hasOwnProperty;var c=(n,e)=\u003e()=\u003e(n\u0026\u0026(e=n(n=0)),e);var y=(n,e)=\u003e()=\u003e(e||n((e={exports:{}}).exports,e),e.exports),b=(n,e)=\u003e{for(var i in e)s(n,i,{get:e[i],enumerable:!0})},l=(n,e,i,m)=\u003e{if(e\u0026\u0026typeof e==\"object\"||typeof e==\"function\")for(let r of u(e))!x.call(n,r)\u0026\u0026r!==i\u0026\u0026s(n,r,{get:()=\u003ee[r],enumerable:!(m=f(e,r))||m.enumerable});return n};var w=(n,e,i)=\u003e(i=n!=null?g(v(n)):{},l(e||!n||!n.__esModule?s(i,\"default\",{value:n,enumerable:!0}):i,n)),j=n=\u003el(s({},\"__esModule\",{value:!0}),n);var o=c(()=\u003e{});var a=c(()=\u003e{});var p=y((X,h)=\u003e{o();a();h.exports=_jsx_runtime});var M={};b(M,{default:()=\u003eR});o();a();var t=w(p());function d(n){let e=Object.assign({p:\"p\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Reinforcement learning (RL)\"}),` aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.\nWith the help of `,(0,t.jsx)(e.em,{children:\"inverse reinforcement learning (IRL)\"}),\" we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Maximum entropy IRL\"}),` is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.\nIn this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.`]})]})}function L(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(d,n)})):d(n)}var R=L;return j(M);})();\n;return Component;","tags":["machine-learning","reinforcement-learning","inverse-reinforcement-learning"],"visibility":"default","content":null}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"-WAzr888L1_oNiBCTp_Kl","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>