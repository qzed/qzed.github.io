<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><title>Blog | Maximilian Luz</title><meta name="description" content="Blog of Maximilian Luz"/><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/de1054d588efc4cb.css" as="style"/><link rel="stylesheet" href="/_next/static/css/de1054d588efc4cb.css" data-n-g=""/><link rel="preload" href="/_next/static/css/3804a2044fcfa42a.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3804a2044fcfa42a.css" data-n-p=""/><link rel="preload" href="/_next/static/css/13d8eb0cee570e71.css" as="style"/><link rel="stylesheet" href="/_next/static/css/13d8eb0cee570e71.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-0d1b80a048d4787e.js"></script><script src="/_next/static/chunks/webpack-73b63104b83bee32.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-25e5079ab4bd6ecd.js" defer=""></script><script src="/_next/static/chunks/pages/_app-56e54c109101a285.js" defer=""></script><script src="/_next/static/chunks/814-38d62155dfe3a73a.js" defer=""></script><script src="/_next/static/chunks/pages/blog-c559a5ba547272c2.js" defer=""></script><script src="/_next/static/kI8q6bitrH_C1nlYeQb8w/_buildManifest.js" defer=""></script><script src="/_next/static/kI8q6bitrH_C1nlYeQb8w/_ssgManifest.js" defer=""></script></head><body><div id="__next"><div class="styles_base__vlVUC"><nav class="navbar_base__qZ3rK"><ul class="navbar_menu__lmA2R"><li class="navbar_item__9W3fH undefined"><a href="/"><div>Home</div></a></li><li class="navbar_item__9W3fH navbar_active__vSOgC"><a href="/blog"><div>Blog</div></a></li></ul></nav><main><div class="styles_post__fDRxW"><div class="styles_column__I7Wru"><div class="styles_text__Fz11m"><div><div><div class="styles_title__6dIZc"><h1><a href="/blog/posts/20220825-irl-maxent">Maximum Entropy Inverse Reinforcement Learning</a></h1></div><div class="styles_meta__v7sr1"><p class="styles_date__lXmvo">August 25, 2022</p><div class="styles_tags__jXUSu"><p class="styles_tag__yYAC7">#<!-- -->machine-learning</p><p class="styles_tag__yYAC7">#<!-- -->reinforcement-learning</p><p class="styles_tag__yYAC7">#<!-- -->inverse-reinforcement-learning</p></div></div><div class="markdown_markdown__28kdH"><p><em>Reinforcement learning (RL)</em> aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.
With the help of <em>inverse reinforcement learning (IRL)</em> we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.</p>
<p><em>Maximum entropy IRL</em> is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.
In this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.</p></div><div><a href="/blog/posts/20220825-irl-maxent">Read more...</a></div></div></div></div></div></div></main><footer class="footer_footer__CKQ67">Copyright Â© 2022 Maximilian Luz</footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"slug":"20220825-irl-maxent","title":"Maximum Entropy Inverse Reinforcement Learning","author":"Maximilian Luz","date":1661385600000,"abstract":"var Component=(()=\u003e{var p=Object.create;var a=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var g=Object.getOwnPropertyNames;var f=Object.getPrototypeOf,u=Object.prototype.hasOwnProperty;var v=(n,e)=\u003e()=\u003e(n\u0026\u0026(e=n(n=0)),e);var x=(n,e)=\u003e()=\u003e(e||n((e={exports:{}}).exports,e),e.exports),y=(n,e)=\u003e{for(var i in e)a(n,i,{get:e[i],enumerable:!0})},m=(n,e,i,s)=\u003e{if(e\u0026\u0026typeof e==\"object\"||typeof e==\"function\")for(let r of g(e))!u.call(n,r)\u0026\u0026r!==i\u0026\u0026a(n,r,{get:()=\u003ee[r],enumerable:!(s=d(e,r))||s.enumerable});return n};var b=(n,e,i)=\u003e(i=n!=null?p(f(n)):{},m(e||!n||!n.__esModule?a(i,\"default\",{value:n,enumerable:!0}):i,n)),w=n=\u003em(a({},\"__esModule\",{value:!0}),n);var o=v(()=\u003e{});var l=x((C,c)=\u003e{o();c.exports=_jsx_runtime});var L={};y(L,{default:()=\u003eI});o();var t=b(l());function h(n){let e=Object.assign({p:\"p\",em:\"em\"},n.components);return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Reinforcement learning (RL)\"}),` aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.\nWith the help of `,(0,t.jsx)(e.em,{children:\"inverse reinforcement learning (IRL)\"}),\" we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.\"]}),`\n`,(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.em,{children:\"Maximum entropy IRL\"}),` is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.\nIn this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.`]})]})}function _(n={}){let{wrapper:e}=n.components||{};return e?(0,t.jsx)(e,Object.assign({},n,{children:(0,t.jsx)(h,n)})):h(n)}var I=_;return w(L);})();\n;return Component;","tags":["machine-learning","reinforcement-learning","inverse-reinforcement-learning"],"visibility":"default","content":null}]},"__N_SSG":true},"page":"/blog","query":{},"buildId":"kI8q6bitrH_C1nlYeQb8w","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>