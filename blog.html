<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/172858f69970fe5d.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/e05f8db85aa918f4.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/81db15d06f37591a.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b34b17fb8764ac71.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-078960448c80a0bb.js" async="" crossorigin=""></script><script src="/_next/static/chunks/472-1acba176e2a8e717.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-a7a6a27a84558edb.js" async="" crossorigin=""></script><script src="/_next/static/chunks/326-976d427cb6019efb.js" async=""></script><script src="/_next/static/chunks/app/layout-7aefb292589726d7.js" async=""></script><script src="/_next/static/chunks/app/blog/page-fee6950aa80b5122.js" async=""></script><title>Blog | Maximilian Luz</title><meta name="description" content="Blog of Maximilian Luz"/><link rel="icon" href="/assets/icon-dark.svg"/><link rel="icon" href="/assets/icon-dark.svg" media="(prefers-color-scheme: light)"/><link rel="icon" href="/assets/icon-light.svg" media="(prefers-color-scheme: dark)"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="home_base__hHa1D"><nav class="navbar_base__ZGSyI"><ul class="navbar_menu__9VAe7"><li class="navbar_item__tSgzS undefined"><a href="/"><div>Home</div></a></li><li class="navbar_item__tSgzS navbar_active__qIXOm"><a href="/blog"><div>Blog</div></a></li></ul></nav><main><div class="styles_post__X_vcU"><div class="styles_column__Rpho_"><div class="styles_text__9QAN_"><div><div><div class="styles_title__tHXpB"><h1><a href="/blog/posts/20220825-irl-maxent">Maximum Entropy Inverse Reinforcement Learning</a></h1></div><div class="styles_meta__o8CyI"><p class="styles_date__FN_ra">August 25, 2022</p><div class="styles_tags__HYypl"><p class="styles_tag__1iSz6">#<!-- -->machine-learning</p><p class="styles_tag__1iSz6">#<!-- -->reinforcement-learning</p><p class="styles_tag__1iSz6">#<!-- -->inverse-reinforcement-learning</p></div></div><div class="markdown_markdown__o1E4A"><p><em>Reinforcement learning (RL)</em> aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.
With the help of <em>inverse reinforcement learning (IRL)</em> we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.</p>
<p><em>Maximum entropy IRL</em> is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.
In this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.</p></div><div><a href="/blog/posts/20220825-irl-maxent">Read more...</a></div></div></div></div></div></div></main><footer class="footer_footer__YtHeK">Copyright © 2023 Maximilian Luz</footer><script src="/_next/static/chunks/webpack-b34b17fb8764ac71.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/172858f69970fe5d.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L2\"\n"])</script><script>self.__next_f.push([1,"3:HL[\"/_next/static/css/e05f8db85aa918f4.css\",\"style\",{\"crossOrigin\":\"\"}]\n4:HL[\"/_next/static/css/81db15d06f37591a.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"5:I[3728,[],\"\"]\n7:I[9928,[],\"\"]\n8:I[1444,[\"326\",\"static/chunks/326-976d427cb6019efb.js\",\"185\",\"static/chunks/app/layout-7aefb292589726d7.js\"],\"\"]\n9:I[6954,[],\"\"]\na:I[7264,[],\"\"]\n"])</script><script>self.__next_f.push([1,"2:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/172858f69970fe5d.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"YpjHvgpbYrUzoSPSSl9Zk\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L6\"],\"globalErrorComponent\":\"$7\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"home_base__hHa1D\",\"children\":[[\"$\",\"$L8\",null,{}],[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[\"$\",\"main\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"not-found_section__DOGpD not-found_error__OPrrT\",\"children\":[\"$\",\"div\",null,{\"className\":\"not-found_column__UyRkF\",\"children\":[\"$\",\"div\",null,{\"className\":\"not-found_text__JVLHJ\",\"children\":[[\"$\",\"p\",null,{\"className\":\"not-found_errorcode__eS_iR\",\"children\":\"404\"}],[\"$\",\"p\",null,{\"className\":\"not-found_errortext__rQxgq\",\"children\":\"Page not found!\"}]]}]}]}]}],\"notFoundStyles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/a6a993177a25134a.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],\"childProp\":{\"current\":[\"$\",\"$L9\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"template\":[\"$\",\"$La\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$Lb\",\"$Lc\",null],\"segment\":\"__PAGE__\"},\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e05f8db85aa918f4.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/81db15d06f37591a.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}],\"segment\":\"blog\"},\"styles\":[]}],[\"$\",\"footer\",null,{\"className\":\"footer_footer__YtHeK\",\"children\":\"Copyright © 2023 Maximilian Luz\"}]]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"1\",{\"children\":\"Blog | Maximilian Luz\"}],[\"$\",\"meta\",\"2\",{\"name\":\"description\",\"content\":\"Blog of Maximilian Luz\"}],[\"$\",\"meta\",\"3\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/assets/icon-dark.svg\"}],[\"$\",\"link\",\"5\",{\"rel\":\"icon\",\"href\":\"/assets/icon-dark.svg\",\"media\":\"(prefers-color-scheme: light)\"}],[\"$\",\"link\",\"6\",{\"rel\":\"icon\",\"href\":\"/assets/icon-light.svg\",\"media\":\"(prefers-color-scheme: dark)\"}]]\n"])</script><script>self.__next_f.push([1,"b:null\n"])</script><script>self.__next_f.push([1,"d:I[8326,[\"326\",\"static/chunks/326-976d427cb6019efb.js\",\"404\",\"static/chunks/app/blog/page-fee6950aa80b5122.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"c:[\"$\",\"main\",null,{\"children\":[[\"$\",\"div\",\"20220825-irl-maxent\",{\"className\":\"styles_post__X_vcU\",\"children\":[\"$\",\"div\",null,{\"className\":\"styles_column__Rpho_\",\"children\":[\"$\",\"div\",null,{\"className\":\"styles_text__9QAN_\",\"children\":[\"$\",\"div\",null,{\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"styles_title__tHXpB\",\"children\":[\"$\",\"h1\",null,{\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/blog/posts/20220825-irl-maxent\",\"children\":\"Maximum Entropy Inverse Reinforcement Learning\"}]}]}],[\"$\",\"div\",null,{\"className\":\"styles_meta__o8CyI\",\"children\":[[\"$\",\"p\",null,{\"className\":\"styles_date__FN_ra\",\"children\":\"August 25, 2022\"}],[\"$\",\"div\",null,{\"className\":\"styles_tags__HYypl\",\"children\":[[\"$\",\"p\",\"machine-learning\",{\"className\":\"styles_tag__1iSz6\",\"children\":[\"#\",\"machine-learning\"]}],[\"$\",\"p\",\"reinforcement-learning\",{\"className\":\"styles_tag__1iSz6\",\"children\":[\"#\",\"reinforcement-learning\"]}],[\"$\",\"p\",\"inverse-reinforcement-learning\",{\"className\":\"styles_tag__1iSz6\",\"children\":[\"#\",\"inverse-reinforcement-learning\"]}]]}]]}],[\"$\",\"div\",null,{\"className\":\"markdown_markdown__o1E4A\",\"children\":[[\"$\",\"p\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"Reinforcement learning (RL)\"}],\" aims to provide a framework for finding the optimal behavior (i.e., an optimal policy) of intelligent agents regarding some environment they interact with by directing them via a reward signal, measuring their performance.\\nWith the help of \",[\"$\",\"em\",null,{\"children\":\"inverse reinforcement learning (IRL)\"}],\" we can try to improve our agents by recovering the reward function (and therefore policy) of an expert, in essence using its domain knowledge for our needs.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"em\",null,{\"children\":\"Maximum entropy IRL\"}],\" is a comparatively simple but clever method of solving the general IRL problem for discrete Markov decision processes.\\nIn this blog post, I will lay out the theoretical foundation of this approach, including the principle of maximum entropy, and derive the maximum entropy IRL algorithm.\"]}]]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$Ld\",null,{\"href\":\"/blog/posts/20220825-irl-maxent\",\"children\":\"Read more...\"}]}]]}]}]}]}]}]]}]\n"])</script></body></html>